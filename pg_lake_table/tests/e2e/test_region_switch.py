import os
import pytest
from utils_pytest import *


def test_region_switch(s3, pg_conn, pgduck_conn, extension):
    # The following URLs point to real files in a real bucket which reside
    # in us-east-2
    url1 = f"s3://aws-public-blockchain/v1.0/btc/blocks/date=2022-01-01/part-00000-477cb938-98c9-4865-9908-4e4a20ddf021-c000.snappy.parquet"
    url2 = f"s3://aws-public-blockchain/v1.0/btc/blocks/date=2022-01-02/*.parquet"
    url3 = f"s3://aws-public-blockchain/v1.0/btc/blocks/date=2022-01-03/*.parquet"

    access_key_id = os.getenv("CDWREGIONTEST_ACCESS_KEY_ID")
    secret_access_key = os.getenv("CDWREGIONTEST_SECRET_ACCESS_KEY")

    # Set up credentials from environment variables (with the wrong region)
    # Otherwise, we sign requests with our nonsense credentials, which S3 rejects
    if access_key_id:
        run_command(
            f"""
            CREATE OR REPLACE SECRET s3pglregiontest (
                TYPE S3,
                KEY_ID '{access_key_id}',
                SECRET '{secret_access_key}',
                SCOPE 's3://aws-public-blockchain',
                REGION 'us-east-1',
                ENDPOINT 's3.amazonaws.com'
            );
        """,
            pgduck_conn,
        )

    # Locally, use ~/.aws/credentials
    else:
        run_command(
            f"""
            CREATE OR REPLACE SECRET s3pglregiontest (
                TYPE S3,
                SCOPE 's3://aws-public-blockchain',
                PROVIDER CREDENTIAL_CHAIN,
                REGION 'us-east-1',
                ENDPOINT 's3.amazonaws.com',
                CHAIN 'config'
            );
        """,
            pgduck_conn,
        )

    pgduck_conn.commit()

    # We will not see the region in glob operations
    result = run_query(
        f"""
        SELECT file FROM glob('{url2}')
    """,
        pgduck_conn,
    )
    assert "s3_region" not in result[0]["file"]

    # Check again for cached region
    result = run_query(
        f"""
        SELECT file FROM glob('{url2}')
    """,
        pgduck_conn,
    )
    assert "s3_region" not in result[0]["file"]

    # We should be able to access the file even if we do not specify region
    run_command(
        f"""
        CREATE FOREIGN TABLE blocks_1 () SERVER pg_lake
        OPTIONS (path '{url1}');
    """,
        pg_conn,
    )

    result = run_query(
        """
        SELECT count(*) FROM blocks_1
    """,
        pg_conn,
    )
    assert result[0]["count"] == 172

    # Other query arguments should not be an issue
    run_command(
        f"""
        CREATE FOREIGN TABLE blocks_2 () SERVER pg_lake
        OPTIONS (path '{url2}?s3_endpoint=s3.amazonaws.com');
    """,
        pg_conn,
    )

    result = run_query(
        """
        SELECT count(*) FROM blocks_2
    """,
        pg_conn,
    )
    assert result[0]["count"] == 158

    # If we explicitly specify the region, we still get an error
    error = run_command(
        f"""
        CREATE FOREIGN TABLE blocks_3 () SERVER pg_lake
        OPTIONS (path '{url3}?s3_region=us-east-1');
    """,
        pg_conn,
        raise_error=False,
    )
    assert "HTTP 400" in error

    pg_conn.rollback()


def test_managed_storage_region(s3, pg_conn, pgduck_conn, extension):
    # Attempt to create an Iceberg table in a different region will fail
    error = run_command(
        f"""
        CREATE TABLE region_iceberg (x int, y int) USING iceberg
        WITH (location = 's3://aws-public-blockchain/region_iceberg');
    """,
        pg_conn,
        raise_error=False,
    )
    assert "can only create Iceberg tables in region" in error

    pg_conn.rollback()

    error = run_command(
        f"""
        SET pg_lake_iceberg.default_location_prefix = 's3://aws-public-blockchain/';
        CREATE TABLE region_iceberg (x int, y int) USING iceberg;
    """,
        pg_conn,
        raise_error=False,
    )
    assert "can only create Iceberg tables in region" in error

    pg_conn.rollback()
